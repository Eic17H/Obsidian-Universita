900 d.C. (X secolo), impero Persiano, Al-Khwarizmi inventa Al-Jabr, l’algebra.  
Dal nome Al-Khwarizmi (e da arithmos, numero) deriva “algoritmo”.  
Un algoritmo è un insieme finito e ordinato di passi univoci eseguibili che definiscono un processo che termina.

Alla fine del ‘600 fu inventata la Pascalina, precursore dei computer.

Computer: Dall’inglese, in origine la professione di fare calcoli per i censi. I macchinari hanno preso questo nome dopo un po’ di tempo dalla loro invenzione.

La macchina analitica di Babbage, l’analytical engine, fu inventata intorno al 1840 per risolvere problemi analitici, cioè matematici. Babbage ci spese l’equivalente di oltre 10 milioni di euro.  
Babbage si rese conto che questo nuovo tipo di macchina si poteva programmare, a differenza delle macchine precedenti, che potevano compiere un solo lavoro.  
La parola computer fu introdotta per questi macchinari negli anni ‘90.

Un altro precursore dei computer era il telaio a schede perforate, che funzionava con lo stesso principio: diamo istruzioni alla macchina perché risolvano problemi per conto nostro.  
Inizialmente, questa era una professione stereotipicamente femminile.  
La prima programmatrice fu Ada Lovelace.

Altre figure molto importanti furono Alonzo Church e Alan Turing.  
Church inventò il λ-calcolo.  
Turing ideò la macchina di Turing, che vedremo l’anno prossimo.  
Insieme, pubblicarono la tesi di Church-Turing.

L’esistenza dell’informatica non dipende dai computer. Senza computer, l’informatica sarebbe rimasta una disciplina sconosciuta, esoterica e presa in giro.

Macchina di Von Neumann del ‘46:  
Nuovo concetto di calcolatore con memoria (per dati e programma), (pensavano in termini di funzioni matematiche)  
Esecutore programmabile di istruzioni = processore  
Oggi caratteristiche tutte banali.

La prima memoria di massa mai inventata: tre fili ortogonali, intrecciati in un anello di ferrite magnetizzata: a seconda di come passa la corrente per i tre fili, l’anello è magnetizzato in senso orario o antiorario, bit 1 o 0. L’anello può cambiare dinamicamente, ma una volta cambiato senso, il nuovo senso rimane (immagazzinato).

Era già stato introdotto un modello fondamentale: fetch, decode, execute, ripeti. Prende un po’ di dati codificati, li decodifica, esegue le istruzioni che contengono, e ricomincia. È talmente fondamentale che si usa ancora oggi.  
Un algoritmo può essere scritto in pseudo-codice, cioè comprensibile da un essere umano.  
Molto diverso è il codice per i computer:  
Tempo fa l’unico modo era il codice assembly, che doveva essere cambiato ogni volta che un programma doveva essere usato su una macchina diversa. Questo rendeva impossibile il porting, cioè l’atto di rendere disponibile un programma su più macchine.

Obiettivo: rendere la programmazione alla portata di molti.  
Questo è legato al concetto moderno di natural interaction: per esempio, imparare a usare uno smartphone è intuitivo, perché c’è un lavoro dietro.  
La soluzione fu l’invenzione dei linguaggi ad alto livello.  
Chomsky, importante linguista, divise le lingue in 4 gruppi, tra cui i linguaggi formali.