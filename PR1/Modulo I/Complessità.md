Un programma complesso pu√≤ andare bene per un insieme di dati piccolo, ma per quelli grandi essere troppo lento.

Il caso medio si denota con $O(n)$, il caso ottimo con $Œ©(n)$ e il caso pessimo con $Œò(n)$, dove $n$ √® la dimensione dei dati.
A noi interessa $O(n)$.

$O(f(n))$ √® la classe di complessit√†.
Leggendo la definizione formale avete il diritto di non capirci niente.
Aiuto ci sono i logaritmi perch√©? Aaa.

Esempio: $n^2+2n+4 ‚àã O(n^2) ‚à¥ ∆é n, c |$ boh vedi slide.
$n^2$ asintoticamente esprime $n^2+2n+4$.
$lim_{n‚Üí‚àû} n^2+2n+4 = lim_{n‚Üí‚àû} n^2 = ‚àû^2$.

| Le classi di complessit√† pi√π comuni sono: | <                             | <                                                                                                                                                                                                                       |
| ----------------------------------------- | ----------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| <br><br><br><br><br><br>üòá                | $O(1)$                        | Ci mette sempre la stessa quantit√† di tempo. Scordatelo.                                                                                                                                                                |
| ^                                         | $O(log(n))$                   | Tempo ‚àù log(n). Tipicamente log‚ÇÇ.<br>log8=3, log1024=10.<br>La differenza tra 8 e 1024 √® molta, quella tra 3 e 10 no.<br>Molto efficiente.<br>Caratteristico della ricerca in set ordinati. Pagine gialle dato il nome. |
| ^                                         | $O(n)$                        | Lineare. Caratteristico della ricerca in set disordinati. Pagine gialle dato il numero di telefono.                                                                                                                     |
| ^                                         | $O(n log(n))$                 | Linearitmico, poco pi√π che lineare.<br>Ancora molto accettabile. Caratteristico dell‚Äôordinamento.                                                                                                                       |
| <br><br><br>üòà                            | $O(n¬≤)$<br>$O(n¬≥)$<br>$O(2‚Åø)$ | Quadratica, cubica, esponenziale, fattoriale.<br>Questi sono pessimi.                                                                                                                                                   |
| ^                                         | $O(n!)$                       | Esplorazione combinatorica di uno spazio di soluzioni.<br>Hai n oggetti, di cui vuoi trovare la combinazione ideale guardandole una per volta. Processo estremamente lungo.                                             |

Vedi la tabella nelle slide.
L‚Äôuniverso ha 10¬π‚Å¥ anni. Il ‚Äú>>‚Äù nella slide significa ‚Äúpi√π di 10¬≤‚Åµ anni‚Äù. Il punto separa le migliaia.
Google deve cercare tra milioni di risultati. La complessit√† dell‚Äôalgoritmo di ricerca √® molto importante.
La radice √® poco pi√π del logaritmo.
Nel grafico, $n!$ √® praticamente verticale.

$Œ©(n)$: Meglio di cos√¨ non si pu√≤ fare, lower bound.
Se $Œ©(n)=nlogn$ e tu mi dici che il tuo $O(n)$ √® $\sqrt{n}$ mi stai dicendo una bugia.
$Œò(n)$: Peggio di cos√¨ non si pu√≤ fare, se ci riesci sei un <span class="pink">caprone puzzolente</span>.

La O(n) andrebbe calcolata sull‚Äôalgoritmo, noi lo facciamo sulla sua traduzione in linguaggio di programmazione, √® equivalente.
Complessit√† di alcune operazioni.

L‚Äôassegnazione √® $O(1)$. Che stia assegnando $5$ o $1000000$ ci vuole la stessa quantit√† di tempo. Anche le operazioni aritmetiche sono a tempo fisso: ci sono circuiti dedicati.
Lo stesso vale per i confronti: `< > = <= >=`.

Quando nei rami di un blocco di scelta ci sono complessit√† diverse, considero la peggiore.
Mi prendo il sicuro.
Per il for, la condizione, l‚Äôincremento e le istruzioni si ripetono, quindi si moltiplicano (credo).
Insomma, vedi le slide, c‚Äô√® tutto.

Per combinare insieme le complessit√† dei singoli pezzi del programma, funziona un po‚Äô come i limiti.
Nelle addizioni, rimuovi la componente pi√π piccola. Nelle moltiplicazioni, si tengono entrambe, ma $O(1)$ √® l‚Äôelemento neutro.
Si scrive $g(n)‚ààO(n)$, non $g(n)=O(n)$.
Se non conosco una funzione la lascio esplicita, tipo $O(n‚ãÖg(n))$.